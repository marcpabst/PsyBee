{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"advanced/","title":"Advanced Usage","text":""},{"location":"advanced/#event-driven-experiment-flows","title":"Event-driven experiment flows","text":"<p>The traditional way to write a PsyBee experiment is to use a loop and submit individual frames to the window. This is simple, provides a high degree of control over stimulus presentation, and is suitable for many experiments. However, while PsyBee is not designed to be a GUI toolkit, it supports some limited event-driven programming.</p> <p>In short, event-driven programming is a programming paradigm in which the flow of the program is determined by events such as user actions (mouse clicks, key presses), sensor outputs, or messages from other programs or threads. In the context of PsyBee, this means that the flow of the experiment is determined by events such as the start of a trial, the end of a trial, or the presentation of a stimulus.</p> <p>Event-driven programming is a powerful form of encapsulation that is useful when dealing with complex, asynchronous systems. It is particularly useful when dealing with systems that are inherently event-driven, such as graphical user interfaces. However, it can make it difficult to deliver frame-accurate control over stimulus presentation and is generally harder to understand than traditional, loop-based programming. Nevertheless, it is a useful tool for experiments (or parts of experiments) that don't require precise timing, like questionnaires or other non-timed tasks.</p> <p>Info</p> <p>In general, all things that can be done with event-driven programming can also be done with traditional, loop-based programming. See the section on dealing with events for more information.</p> <ul> <li> <p>Here is a diagram of a single trial in a traditional, non-event-driven experiment:</p> <pre><code>flowchart TD\n    A[Start of trial] --&gt; B[New frame]\n    B --&gt; C[Add stimulus to frame]\n    C --&gt; D[Present frame]\n    D --&gt; E[Check for response]\n    E --&gt; |No response| B\n    E --&gt; |Response|F[End trial]</code></pre> </li> <li> <p>And here is a diagram of a single trial in an event-driven experiment:</p> <pre><code>flowchart TD\n    A[Start of trial] --&gt; B[Show stimulus until response]\n    B --&gt; |Response|C[End trial]</code></pre> </li> </ul> <p>Todo</p> <p>Add more information about event-driven programming.</p>"},{"location":"api/","title":"API","text":""},{"location":"api/window/","title":"Window","text":""},{"location":"background/colours/","title":"Colours and Gamma Correction","text":"<p>This article proides background information on colour spaces and gamma correction. PsyBee currently does not provide full colour management support, but you can specify the encoding used and you can perform gamma calibration yourself.</p> <p>A colour space is a mathematical model that describes how colours can be represented as tuples of numbers. Since human vision is trichromatic (i.e. we have three types of colour receptors in our eyes), most colour spaces are tristimulus colour spaces, meaning that colours can be represented as a combination of three numbers.</p>"},{"location":"background/colours/#useful-colour-spaces","title":"Useful colour spaces","text":""},{"location":"background/colours/#rgb-colour-spaces","title":"RGB colour spaces","text":"<p>RGB colour spaces are tristimulus colour spaces that are based on defining values for red, green, and blue (so-called \"primaries\") and a white point. They are the most ubiquitous colour spaces in computer graphics and will probably be the most familiar to you.</p> <p>Terminology</p> <p>It can be slightly confusing that the term \"RGB colour space\" is used to refer to both the concept of a colour space represented by red, green, and blue, and to a specific colour space that is defined by a set of specific red, green, and blue primaries and a white point (like sRGB or Adobe RGB). For sake of clarity, you should always clearly state which RGB colour space you are referring to.</p> <p>The three most important concepts in RGB colour spaces are: the primaries, the white point, and the encoding.</p> <p>The primaries are the three colours that are used to represent all other colours. Any given, clearly defined colour space will have a distinct set of primaries. For example, the sRGB colour space uses the following primaries (in CIE xyY colour space):</p> <ul> <li>Red: (X = 0.64, Y = 0.33)</li> <li>Green: (X = 0.30, Y = 0.60)</li> <li>Blue: (X = 0.15, Y = 0.06)</li> </ul> <p>The white point is the colour of a perfectly white surface, i.e. when all three primary colours are present in equal amounts. White points are usually defined in terms of their spectral power distribution, and the most commonly used white point is the CIE standard illuminant D65, which is characterized by a colour temperature of approximately 6500 K <sup>3</sup> <sup>4</sup>.</p> <p>The encoding is the transfer function that is used to convert the RGB values to light intensity (also called electro-optical transfer function, see below). The encoding can be linear or non-linear.</p> <p>Confusingly, some definitions of certain RGB colour spaces include the encoding, while others do not. For example, the sRGB colour space is usually used with with so-called \"sRGB transfer function\" (a non-linear encoding), but you sometimes the sRGB primaries and white point are used with a linear encoding (often called \"linear sRGB\"). Even more confusingly, not all implementations of sRGB use the same transfer function. The sRGB transfer function is formally defined by an ICE standard, and it is a piecewise function that combines a linear segment with a power function with an exponent of 2.4. However, some implementations of sRGB use a slightly different transfer function. For example, many displays approximate the sRGB transfer function using a power function with an exponent of 2.2. Annoyingly, some (even professional) software packages have needlessly invented their own transfer functions for sRGB, which can cause confusion and compatibility issues.<sup>5</sup></p> <p>RGB colour spaces are extremely useful for computer graphics because they are very intuitive and easy to understand. Moreover, almost all modern display technologies are based on emitting red, green, and blue light, which makes RGB colour spaces a natural choice for representing colours that are to be displayed on a screen. Some common RGB colour spaces are:</p> <ul> <li>sRGB (the standard colour space for most consumer applications)</li> <li>Adobe RGB (a colour space that is used in professional photography and printing)</li> <li>Display P3 (a colour space that is used in Apple's devices)</li> <li>Rec. 709 (a colour space that is used in HDTV systems)</li> <li>Rec. 2020 (a colour space that is used in UHDTV systems)</li> </ul>"},{"location":"background/colours/#cie-1931-xyz-colour-space","title":"CIE 1931 XYZ colour space","text":"<p>The CIE XYZ colour space captures all perceivable colours through three components: <code>X</code>, <code>Y</code>, and <code>Z</code>. Where <code>Y</code> denotes luminance, while <code>X</code> and <code>Z</code> denote colour information. Because it's device-independant, it is often used a a universal standard for colour representation. Also, the CIE XYZ colour space is the foundation for many other colour spaces, including the CIE xyY colour space, which is a transformation of XYZ to a more perceptually uniform space.</p>"},{"location":"background/colours/#lms-colour-space","title":"LMS colour space","text":"<p>The LMS colour space is a colour space that is based on the response of the three types of cones in the human retina. It is a linear colour space, meaning that the values are proportional to the number of photons that are absorbed by the cones. This makes it a very useful colour space for vision scientists, because it allows us to represent colours in a way that is directly related to the physical properties of the light that is absorbed by the cones. However, it is not a very intuitive colour space for most people, and because it is linke to the spectral properties of both the display and the observer, it can be difficult to work with in practice.</p>"},{"location":"background/colours/#converting-numbers-to-light-intensity","title":"Converting numbers to light intensity","text":"<p>How do we go from three colour values to light intensity? This is where the concept of the electro-optical transfer function (EOTF) comes in.</p> <p>The electro-optical transfer function (EOTF) is a function that describes how the RGB values are converted to light intensity. In other words, it describes how the RGB values are converted to actual number of photons emitted by the display. In the simplest case, the EOTF is a linear function that simply scales the RGB values by a constant factor. For example, this would mean that an RGB value of <code>(100, 100, 100)</code> would result in twice as much light being emitted as an RGB value of <code>(50, 50, 50)</code>.</p> <p>However, most displays use a non-linear EOTF. This is for both historical, perceptual, and technical reasons.</p> <p>The following section contains some semi-techincal and vaguely interesting information about the history of display technology. Feel free to skip it if you are not interested in this.</p> <p>Until the late 1990s, most displays were based on the cathode ray tube (CRT) technology. In CRT displays, an electron beam is used to excite phosphors on the screen, which then emit light. The brightness of the light emitted is a function of the voltage difference between the electron gun (cathode) and the grid (anode). This function is, as it turns out, highly non-linear (it is a somewhat common misconception that the nonlinearity of a CRT is due to the phosphor, but this is not the case; in fact, the nonlinearity is due to the physics of the electron beam while the phosphor response is actually quite linear) <sup>1</sup>.</p> <p>Here is really interesting part: If you are fammiliar with basic human physiology, you might be aware that the human visual system also behaves non-linearly (in fact, this is a principle that holds for most sensory systems and was first described by Gustav Theodor Fechner and Ernst Heinrich Weber in the 19th century).</p> <p>This means that the perceived brightness of a light is not proportional to the actual number of photons emitted, but roughly follows a power law. This is an amazing coincidence, because it means that the non-linear response of the human visual system is almost perfectly matched to the inverse of the non-linear response of the CRT.</p> <p>Why is this important?</p> <p>When encoding colour values in a limited number of bits per channel (e.g. 8 bits per channel or 256 values per channel as is common in computer graphics), it would be a bad idea to spread the values uniformly across the entire range.</p> <p>Example: Linear encoding</p> <p>Assuming 256 values per channel, a linear encoding would mean that the difference between the darkest and the brightest value is 255. This would mean that the difference between the darkest and the second darkest value is 1/255. This might sound like a small difference, but it is actually a huge difference in terms of perceived light intensity. In fact, the ratio of the perceived intensities of the darkest and the second darkest value is over 6%, well above the threshold of visibility.</p> <p>On the other hand, the difference between the brightest and the second brightest value is also 1/255, but this is a much smaller difference in terms of perceived light intensity. In fact, the ratio of the perceived intensities of the brightest and the second brightest value is less than 0.3%. That is completely invisible to the human eye!</p> <p>but rather concentrate them where the human visual system is most sensitive. In practive, one shoud use more of the 256 values to represent darker shades, and fewer to represent brighter shades. Mathematically, this mweans that we should use an encoding function that is the inverse of the human visual system's decoding function.</p> <p>The fact that the CRT's non-linear response is almost perfectly matched to the human visual system's non-linear response is a happy coincidence, and it means that it was possible to exploit the CRT's non-linear response to encode the colour values in a way that is more efficient than a linear encoding.</p> <p>Fast forward to today, and we no longer use CRT displays. However, the non-linear EOTF has stuck around and has been adopted as the standard for most modern displays. This is because it is still a very efficient way to encode colour values, and because it is compatible with the vast amount of content that has been created for and using CRT displays.</p>"},{"location":"background/colours/#srgb-colour-space","title":"sRGB colour space","text":"<p>The most commonly used EOTF today is the sRGB transfer function.</p> <p>The sRGB color space, which is the standard color space for numerous consumer applications, was developed by HP and Microsoft in 1996 ans was later regosnised by the International Electrotechnical Commission (IEC) <sup>2</sup>. sRGB incorporates the Rec. 709 primaries, which were originally introduced in 1990 for HDTV systems under the ITU-R Recommendation BT.709. The white point in sRGB is the CIE standard illuminant D65, characterized by a color temperature of approximately 6500 K<sup>3</sup>.</p> <p>The transfer function of sRGB is non-linear and piecewise linear, approximating a gamma of 2.2:</p> <pre><code>if c &lt;= 0.0031308\n   c * 12.92\nelse\n  1.055 * c^(1.0 / 2.4) - 0.055\n</code></pre> <p>where <code>c</code> is the RGB in the range [0.0, 1.0].</p> <p>As described above, this encoding method was initially tailored for the gamma characteristics of CRT displays. Coincidentally, it also mimics the response of the human visual system to light in daylight conditions, making linearly spaced RGB values correspond to perceived linear light intensity.</p>"},{"location":"background/colours/#gamma-correction","title":"Gamma correction","text":"<p>Terminology</p> <p>Note that the term \"gamma correction\" might be used slightly differently in other contexts.</p> <p>When vision scientists talk about \"gamma correction\", they are usually referring to the process of compensating for the non-linear transfer function of the display. The goal of gamma correction in this context is to ensure that the light intensity emitted by the display is proportional to the RGB values that are sent to the display. This is important because it means that the RGB values can be interpreted as light intensity.</p> <p>It is important to note that all modern graphics libraries and hardware perform some sort of gamma correction automatically because it is a fundamental part of the display pipeline to enure correct colour and alpha blending. However, by default, values are then automatically re-encoded into a (usually non-linear) colour space before being sent to the display.</p> <p>If your goal is to present stimuli with a specific light intensity, you have a number of options to deal with this:</p> <ol> <li> <p>You either trust the display manufacturer to have done a good job at calibrating the display or you calibrate the display yourself, using the tools privded by your operating system or the display manufacturer. You then provide the correct colour values in the correct (linear!) colour space, and let the rendering pipeline take care of the rest. This means basically telling your GPU to skip the conversion into linear space and then have the linear values correctly encoded into the display's colour space. This is the most common approach when working colour managed applications. However, it requires that you have a well-calibrated display and that your operating system, graphics API, and GPU drivers all correctly support colour management.</p> </li> <li> <p>You can perform the gamma correction yourself, either by transforming colours before you pass them to the rendering pipeline, by setting a LUT in the GPU set through your operating system, or by correcting it yourself before the framebuffer is sent to the display (either using a transfer function in a fragment shader or by using a LUT in the GPU).</p> </li> </ol> <p>PsyBee allows you to combine these two approaches. By default, your operating system and GPU drivers will take care of the gamma correction, but you can also correct for innacuracies in the display's gamma correction by performing the gamma correction yourself. This basically means that we provide a mapping from the theoretically expected light intensity to the actual light intensity emitted by the display. Note that this approach is slightly different from the traditional approach to gamma correction, which is to provide a mapping from whatever colour space you are using to the actual light intensity emitted by the display.</p> <p>PsyBee provides a number of tools to help you calibrate your display, and to ensure that the gamma correction is performed correctly.</p> <ol> <li> <p>Charles Poynton, A Technical Introduction to Digital Video. New York: Wiley, 1996.\u00a0\u21a9</p> </li> <li> <p>The sRGB colour space is defined by the International Electrotechnical Commission (IEC) in the standard IEC 61966-2-1:1999.\u00a0\u21a9</p> </li> <li> <p>Following the re-definition of several physical constants in 1968 by the International Committee for Weights and Measures (CIPM), there was a minor shift in the Planckian locus. As a result, the CIE standard illuminant D65 is not precisely at 6500 K, but rather at 6504 K.\u00a0\u21a9\u21a9</p> </li> <li> <p>If you are wondering why this white point was chosen, it apparently matches the colour of normal daylight in western/central Europe.\u00a0\u21a9</p> </li> <li> <p>Will the Real sRGB Profile Please Stand Up?, https://ninedegreesbelow.com/photography/srgb-profile-comparison.html \u21a9</p> </li> </ol>"},{"location":"background/graphicspipeline/","title":"Modern Graphics Pipelines","text":"<ul> <li>Traditional Graphics Pipeline     <pre><code>graph TD\n    A[Application] --&gt;|Submit Draw Calls| B[Acquire Back Buffer]\n    B --&gt;|Render to| C[Back Buffer]\n    C --&gt;|Swap Buffers| D[Front Buffer]\n    D --&gt;|Display| E[Screen]</code></pre></li> <li> <p>Modern Graphics Pipeline</p> <pre><code>graph TD\n    A[Application] --&gt;|Submit Draw Calls| B[Acquire Image]\n        B --&gt;|Record Commands| C[Command Buffer]\n            C --&gt;|Submit to| D[Graphics Queue]\n                D --&gt;|Execute Commands| E[GPU]\n                    E --&gt;|Present Image| F[Presentation Engine]\n                        F --&gt;|Display| G[Screen]</code></pre> </li> </ul>"},{"location":"concepts/colours/","title":"Colours","text":"<ul> <li> The rays, to speak properly, are not coloured. In them there is nothing else than a certain power and disposition to stir up a sensation of this or that colour. <sub>Isaac Newton (1704), Opticks</sub></li> </ul> <p>Correct handling of colours is important for investigating many interesting questions in cognitive and perceptual science. Unfortunately, colours and colour spaces are complex and can be difficult to understand and work with. This document is intended to provide a brief overview over the handling of colours in the PsyBee library. </p> <p>Please read the background information on colours and colour spaces if you are interested in a more detailed discussion of the topic.</p> <p>Key points:</p> <ul> <li>There is currently no dedicated support for full colour managment in the PsyBee library.   As such, the library is limited to working with colours in the RGB colour space using your monitor's primaries. Crucially, this puts the responsibility on the user to ensure that colours are handled correctly.</li> <li>However, the library provides a number of tools to help you work with colours in the RGB colour space.   This mainly concerns the RGB encoding functions, describing the non-linear relationship between the RGB values as defined in a particular RGB colour space and the actual light emitted by the monitor. On top of that, the library provides a way to correct for non-linearities in the monitor's gamma curve (that deviates from the standard sRGB gamma curve). Taken, together, these tools allow for what is often called \"gamma correction\".</li> </ul>"},{"location":"concepts/colours/#gamma-correction","title":"Gamma correction","text":"<p>Imagine your goal is to display four grey squares with different intensities on your monitor, with luminance values spaces equidistantly between 0 and 1. You might be tempted to simply set the RGB values of the squares to the corresponding luminance values. So, for example, the RGB values for the squares with luminance values of 0.25, 0.5, 0.75, and 1.0 would be <code>rgb(0.25, 0.25, 0.25)</code>, <code>rgb(0.5, 0.5, 0.5)</code>, <code>rgb(0.75, 0.75, 0.75)</code>, and <code>rgb(1.0, 1.0, 1.0)</code>, respectively. Once you're done, you might notice that the squares do actually appear to linearly increase in brightness. This is great, right?</p> <p>Well, not quite. As you might know, human perception of brightness is not linear. So if stimuli appear to increase linearly in (perceivded) brightness, they are most likely not actually increasing linearly in (physical) luminance. The reason for this is that the relationship between the RGB values and the actual light emitted by the monitor is non-linear. This non-linear relationship is often referred to as the monitor's gamma curve or, more correctly, as the colour space's encoding function.</p> <p>The process of compensating for the non-linear relationship between the RGB values and the actual light emitted by the monitor is usually called gamma correction. </p>"},{"location":"concepts/events/","title":"Events","text":"<p>Events are raised whenever something interesting happens in the environment, such as a key press, mouse movement, or a touch event. In psychophusics, events are handled by the <code>Event</code> class. There are three types of events that can be raised:</p> <ul> <li>Input events: These events are raised when the user interacts with the experiment using the keyboard, mouse, or touch screen.</li> <li>Window events: These events are raised when the window is resized, moved, or closed.</li> <li>Device events: These result from external devices, and are independent of the window.</li> </ul> <p>Some external triggers can result in multiple events being raised at the same time. For example, a moving a physical mouse can result in both an input event and a device event being raised. This is intentional, as one pertains to the movement of the cursor and the other to the movement of the device (crucially, your operating system might apply transformations to the cursor movement, such as acceleration or inversion, which are not reflected in the device movement). Similarly, a key press can result in both an input event and a device event being raised (but note that windows will only receive input events if they are focused, while device events are independent of focus). It always depends on the specific requirements of your experiment which events you want to handle. </p>"},{"location":"concepts/events/#handling-events","title":"Handling events","text":"<p>There are two ways to handle events in PsyBee: polling and callbacks. Polling is the process of checking for events at regular intervals, while callbacks are functions that are called when an event occurs. Both methods have their advantages and disadvantages, and the best method to use depends on the specific requirements of your experiment.</p> <ul> <li>Polling: You can poll for events by creating a new <code>EventReceiver</code> object and calling its <code>poll</code> method. This will return a list of all events that have occurred since the last call to <code>poll</code>. This method is useful when you need to handle events in a loop.</li> <li>Callbacks: By adding event handlers through <code>add_event_handler</code> to a <code>Window</code> (or, with certain limitations, to a <code>Stimulus</code>), you can register a callback function that will be called whenever an event occurs. This method is useful when you need to handle events as they occur, rather than in a loop.</li> </ul>"},{"location":"concepts/events/#polling-for-events","title":"Polling for events","text":"<p>... some text here ...</p> <p>Tip</p> <p>You can create as many <code>EventReceiver</code> objects as you like, and all of them will receive the same events independently of each other and in the same order. This can be useful if you want to handle events in different parts of your experiment in different ways. Note that currently, only 10.000 events are stored in the event queue, so if you don't poll for events for a long time, you might miss some events.</p> <p>Examples:</p> <pre><code>event_receiver = window.create_event_receiver()\n\nwhile True:\n    for event in event_receiver.poll():\n        if event.type == EventType.KeyPressed:\n            print(f\"Key {event.key} was pressed\")\n</code></pre>"},{"location":"concepts/events/#handling-events-with-callbacks","title":"Handling events with callbacks","text":"<p>Warning</p> <p>When using callbacks, make sure to keep the callback functions as short as possible. This is especially important when using the Python API, as long-running callback functions can lock the GIL and prevent other parts of the experiment from running.</p> <p>A callback is a function that is called when an event occurs. You can register a callback function using the <code>add_event_handler</code> method on a <code>Window</code> object. You can also register a callback function on a <code>Stimulus</code> object, but certain limitations apply (see below). The callback function should take a single argument, which is an <code>Event</code> object. The <code>Event</code> object contains information about the event that occurred, such as the type of event, the key that was pressed, or the position of the mouse.</p>"},{"location":"concepts/events/#adding-event-handlers-to-windows","title":"Adding event handlers to windows","text":"<p>You can add event handlers to windows by calling the <code>add_event_handler(kind, callback)</code> method on a <code>Window</code> object. The <code>kind</code> argument should be one of the values from the <code>EventKind</code> enum, and the <code>callback</code> argument should be a function that takes an <code>Event</code> object as its only argument. The callback function will be called as soon as possible whenever an event of the specified kind occurs.</p> <p>Examples:</p> <pre><code>def on_key_pressed(event):\n    if event.key == Key.Enter:\n        # print a message when the \"Enter\" key is pressed\n        print(\"Enter key was pressed\")\n\n# register the callback function\nwindow.add_event_handler(EventType.KeyPressed, on_key_pressed)\n</code></pre>"},{"location":"concepts/events/#adding-event-handlers-to-stimuli","title":"Adding event handlers to stimuli","text":"<p>You can also add event handlers to stimuli, but because it is hard to correlate frame-by-frame stimulus presentation with precise event timing, this only works when stimuli are added directly to the window (and not to a frame). Nonetheless, this can be useful in some cases, for example when you want to handle events that are not extremely time-sensitive (like clicks on a button).</p> <p>Examples:</p> <pre><code># Create a button stimulus\nbutton = psy.ButtonStimulus(window, psy.Rectangle(100, 100, 200, 200))\n\n# Make the button change color when it is hovered over\ndef on_button_pressed(event):\n    button.set_color(psy.Color(1, 0, 0))\n\nbutton.add_event_handler(EventType.MouseButtonPressed, on_button_pressed)\n\nwindow.add_stimulus(button)\n</code></pre> <p>Note</p> <p>For positional stimuli, the list of stimuli is traversed in the reverse order of their addition to the window. This means that the last stimulus added to the window will be the first to receive events (as it is \"on top\" of all other stimuli). On an implementation level, stimuli then can \"decide\" whether to \"consume\" an event or pass it on to the next stimulus in the list. This is useful to make some stimuli \"invisible\" to certain events, or to make some stimuli \"consume\" an event so that it is not passed on to other stimuli.</p>"},{"location":"concepts/frame/","title":"Windows and Frames","text":""},{"location":"concepts/frame/#window","title":"Window","text":"<p>A <code>Window</code> is an abstraction that represents the display screen on which visual stimuli are presented. It is the primary interface for creating and managing the visual output of a PsyBee experiment. The window provides a canvas on which you can draw visual stimuli, such as shapes, images, and text, and it handles the rendering of these stimuli to the screen. The window also provides methods for handling input events, such as keyboard, mouse and touchscreen interactions.</p> <p>While usually you will find it more useful to add stimuli to a <code>Frame</code> object, and then submit the frame to the window (see below), you can also add stimuli directly to the window. These stimuli will be automatically added to the any frame that is submitted to the window. This can be helpful when you don't need frame-by-frame control over the stimuli, or when you want to add stimuli that are present throughout longer periods of time. This also allows to add event handlers to individual stimuli, which provides an elegant way to handle events that are tied to specific stimuli (e.g. a button that can be hovered over and clicked).</p> <p>Warning</p> <p>For technical reasons, all stimuli must be associated with a window. This is because logically, different windows can be associated with different graphic adapters, and stimuli need to be associated with the same graphic adapter as the window they are rendered on. If you try to add a stimulus to a window that is different from the window it was created on, an error will be raised.</p>"},{"location":"concepts/frame/#creating-a-window","title":"Creating a Window","text":"<p>When an experiment is started, an <code>ExperimentManager</code> object is passed to the experiment function as the sole argument. The <code>ExperimentManager</code> object provides a method <code>create_default_window</code> that can be used to create a window with default settings.</p> <pre><code>from psybee import run_experiment\n\ndef my_experiment(experiment_manager):\n    window = experiment_manager.create_default_window()\n    # Add stimuli and event handlers to the window\n    # ...\n    window.close()\n\nrun_experiment(my_experiment)\n</code></pre>"},{"location":"concepts/frame/#window-settings","title":"Window Settings","text":"<p>In most cases, you will want to customize the settings of the window to match the requirements of your experiment. You can do this by passing a <code>WindowOptions</code> object to the <code>create_window</code> method. The <code>WindowOptions</code> object allows you to specify various settings for the window, such as the screen resolution, refresh rate, and whether the window should be fullscreen or windowed. The <code>ExperimentManager</code> will then do its best to create a window that matches the specified settings (or else raise an error if it is not possible).</p> <p>The following window options are available:</p> <ul> <li><code>Windowed</code>: Create a windowed window with a specific resolution (or default resolution if none is specified) - only supported on desktop platforms.</li> <li><code>FullscreenExact</code>: Create a fullscreen window with a specific resolution and refresh rate (or default values if none are specified) on a specific monitor (or the primary monitor if none is specified). If you specify a resolution or refresh rate that is not supported by the monitor, an error will be raised.</li> <li><code>FullscreenHighestRefreshRate</code>: Create a fullscreen window with the highest refresh rate that is supported by the monitor and matches the specified resolution (or default resolution if none is specified) on a specific monitor (or the primary monitor if none is specified). If you specify a resolution that is not supported by the monitor, an error will be raised.</li> <li><code>FullscreenHighestResolution</code>: Create a fullscreen window with the highest resolution that is supported by the monitor and matches the specified refresh rate (or default refresh rate if none is specified) on a specific monitor (or the primary monitor if none is specified). If you specify a refresh rate that is not supported by the monitor, an error will be raised.</li> </ul> <pre><code>from psybee import run_experiment, window_options\n\ndef my_experiment(exp_manager):\n    win_opts = window_options.FullscreenExact(\n        resolution=(1920, 1080), \n        refresh_rate=60)\n    window = exp_manager.create_window(win_opts)\n\n    # Do stuff with the window\n\n    window.close()\n\nrun_experiment(my_experiment)\n</code></pre>"},{"location":"concepts/frame/#frame","title":"Frame","text":"<p>A <code>Frame</code> refers to a single image within a sequence of images that collectively form the visual output on a display. It is a fundamental unit that represents the entire scene at a specific instance, encapsulating all graphical elements such as objects, textures, colors, and lighting. On a fundamental level, frames are rendered by the graphics processor which turns thw abstract descriptions of what is supposed to be shown on screen into pixels arranged in a grid pattern, each pixel containing color information that contributes to the overall image.</p> <p>In the context of PsyBee, a <code>Frame</code> holds the visual stimuli that are presented on the screen at a specific point in time. To show any stimuli on the screen, you need to first obtain a <code>Frame</code> object, add the stimuli to it, and then <code>submit</code> the frame back to the window. The window will then display the frame on the screen. It depends on the window's refresh rate how often you will need to do this, but typically, you will need to submit a new frame every 16.67 milliseconds (for a 60 Hz display).</p>"},{"location":"concepts/sizes/","title":"Sizes","text":"<ul> <li> When you can measure what you are speaking about, and express it in numbers, you know something about it; but when you cannot measure it, when you cannot express it in numbers, your knowledge is of a meagre and unsatisfactory kind. <sub>Lord Kelvin (1824-1907), in a lecture to the Institution of Civil Engineers</sub></li> </ul> <p>Most functions in psychopysics accept both numerical scalar values (such as <code>int</code> or <code>float</code> - these will always be interpreted as pixels) and the <code>Size</code> type. Size types are used to represent physical quantities in PsyBee, and are used to ensure that the units of the quantities are consistent. For example, the <code>Size</code> type can be used to represent the size of a stimulus in degrees of visual angle.</p> <p>Once you have brought the <code>PsyBee.size</code> module into your namespace, you can create <code>Size</code> values either by calling the constructor functions, ot by using the shorthand syntax. For example, you can create a <code>Size</code> value representing 100 pixels like this:</p> <pre><code>from psybee.size import px\n\nwidth = px(100) # using the constructor function, or\nwidth = 100*px # using the shorthand syntax\n</code></pre> <p>The following units are currently supported:</p> <ul> <li><code>px</code>: pixels</li> <li><code>pt</code>: points</li> <li><code>deg</code>: degrees of visual angle</li> <li><code>sw</code>: fraction of screen width</li> <li><code>sh</code>: fraction of screen height</li> <li><code>deg</code>: degrees of visual angle</li> <li><code>cm</code>: centimeters</li> <li><code>mm</code>: millimeters</li> <li><code>m</code> : meters</li> <li><code>inch</code>: inches</li> </ul> <p>Info</p> <p>Note that for correct results, you will need to set the correct physical size of the screen and the viewing distance. This will ensure that physical units like centimeters or degrees of visual angle are correctly converted to pixels for rendering.</p>"},{"location":"concepts/sizes/#size-arithmetic","title":"Size arithmetic","text":"<p>You can add and subtract <code>Size</code> values, even if they not have the same units. You can also multiply and divide <code>Size</code> values by scalar values. For example:</p> <pre><code>from psybee.size import px, cm, sw\n\nwidth = 1*sw # full screen width\nleft_margin = 2*cm # 2 centimeters\n\nnew_width = width - 2*left_margin # full screen width - margins\n</code></pre> <p>Warning</p> <p>You cannot multiply or divide <code>Size</code> values by other <code>Size</code> values. This is because the result of such an operation would change the dimension of the value, so that the result would no longer represent a size. For example, multiplying two pixel values would result in a pixel squared value, which is a measure of area, not a size. </p>"},{"location":"concepts/sizes/#converting-between-units","title":"Converting between units","text":"<p>You can convert <code>Size</code> values between units using the <code>to</code> method. You can then extract the numerical value of the converted size using the <code>value</code> attribute. For example:</p> <pre><code>from psybee.size import px, cm\n\nwidth = 100*px\nwidth_cm = width.to(cm) # convert to centimeters\nwidth_cm.value # obtain the numerical value\n</code></pre>"},{"location":"concepts/visual-stimuli/","title":"Visual stimuli","text":"<p>A visual stimulus is a visual object that is presented to a participant in an experiment. Visual stimuli can be images, videos, or text. </p>"},{"location":"concepts/visual-stimuli/#introduction","title":"Introduction","text":""},{"location":"concepts/visual-stimuli/#image-stimuli","title":"Image stimuli","text":""},{"location":"concepts/visual-stimuli/#video-stimuli","title":"Video stimuli","text":""},{"location":"concepts/visual-stimuli/#text-stimuli","title":"Text stimuli","text":""},{"location":"getting-started/","title":"Quick Start","text":"<ul> <li>Installation   Install PsyBee using your favourite package manager like <code>pip</code> or <code>conda</code> or include it in your <code>pyproject.toml</code> file.</li> <li>Your first experiment   Create and run your first experiment using PsyBee in less than 5 minutes.</li> <li>Concepts   Learn about the core concepts of PsyBee like colours, stimuli, and windows.</li> <li>API Reference     Detailed documentation of all functions and classes in the PsyBee library.</li> <li>Examples     Explore a variety of examples to get you started with your own experiments.</li> <li>Background information     Gamma correction, colour spaces, and other background information.</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#using-your-favourite-package-manager","title":"Using your favourite package manager","text":"<p>PsyBee can be installed from PyPI using <code>pip</code>, from Conda using <code>conda</code>, or from GitHub using <code>pip</code>. If you instal from PyPI or Conda, a precompiled binary wheel will be downloaded (if available for your platform). If you install from GitHub, the library will be compiled from source. This is also true if there are no precompiled binaries available for your platform. All external dependencies are included in the wheel, so you usually don't need to worry about installing them separately.</p> <p>It is generally recommended to install PsyBee into a virtual environment. This can be done using <code>python -m venv</code> or <code>conda</code>. Please refer to the Python documentation or the Conda documentation for more information.</p> <p>Note</p> <p>We currently provide precompiled binaries for Windows (x64), Mac (arm64), Linux (x64), iOS (arm64), and Android (arm64). If you want to compile the library from source, you will need a number of dependencies. Please refer to the Rust installation guide for more information.</p>  PyPI Conda GitHub <pre><code>pip install psybee\n</code></pre> <pre><code>conda install -c conda-forge psybee\n</code></pre> <pre><code>pip install git+https://github.com/marcpabst/psybee/\n</code></pre> <p>Thats it - you're all set to write your first PsyBee experiment!</p>"},{"location":"getting-started/installation/#using-pyprojecttoml","title":"Using pyproject.toml","text":"<p>Alternatively, if you're using Briefcase or another packaging tool that supports <code>pyproject.toml</code>, you can add the following to your <code>pyproject.toml</code> file. Note that Poetry has a different format for specifying dependencies, amd you will need to refer to the Poetry documentation for more information.</p> <pre><code>[project]\ndependencies = [\n    \"psybee\"\n]\n</code></pre>"},{"location":"getting-started/installation/#using-cargo","title":"Using Cargo","text":"<p>The <code>psybee</code> crate (Rust) can also be installed from Crates.io using <code>cargo</code>, or from GitHub using <code>cargo</code>. Hoewever, there is currently limited documentation available for the Rust API. The only external dependency you will need is gstreamer-1.0 (with the execption of iOS, Android, and the web, where gstreamer-assocated features are disabled).</p> <pre><code>cargo add PsyBee\n</code></pre>"},{"location":"getting-started/tutorial1/","title":"Your first experiment","text":"<p>This section will guide you through the process of creating your first PsyBee experiment using the <code>PsyBee</code> library. We will create a simple experiment that presents a white screen for 1 second, followed by a black screen for 1 second. This will be repeated 10 times.</p>"},{"location":"getting-started/tutorial1/#step-1-install-the-library","title":"Step 1: Install the library","text":"<p>Please refer to the installation guide for instructions on how to install the <code>PsyBee</code> library.</p>"},{"location":"getting-started/tutorial1/#step-2-create-a-new-python-file","title":"Step 2: Create a new Python file","text":"<p>Create a new Python file called <code>first_experiment.py</code> and open it in your favourite text editor.</p>"},{"location":"getting-started/tutorial1/#step-3-write-the-experiment","title":"Step 3: Write the experiment","text":"<p>First, we need to import the <code>PsyBee</code> library:</p> <pre><code>import PsyBee as psy\n</code></pre> <p>Next, we need to create the experiment function. This function will create a window, present a white screen for 1 second, present a black screen for 1 second, and then close the window. This will be repeated 10 times.</p> <pre><code>def my_experiment(wm):\n\n    # create a window with default settings\n    window = wm.create_default_window()\n</code></pre>"}]}